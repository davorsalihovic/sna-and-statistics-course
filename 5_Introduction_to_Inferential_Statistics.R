# 5. INTRODUCTION TO INFERENTIAL STATISTICS ------------
#    Davor Salihovic, University of Antwerp                                                 
#                      davor.salihovic@uantwerpen.be

# Clean the R environment
rm(list = ls())

library(igraph); library(ggplot2); library(coin); library(boot)
library(MKinfer); library(WRS2); library(car); library(pastecs)
library(gridExtra); library(DescTools); library(faux)

library(ggm)
library(QuantPsyc)

#### DATA ----
load("data3.RData")

# Let's simulate a study and generate some data from our original set.
  # Let's say we are investigating how many cups of coffee people drink per week
  # We need data containing information on individuals' sex and number of friends.
data <- data.frame(id = V(g.simple)$name, # the name of each subject
                        degree = degree(g.simple, mode = "total"), # their degree (i.e. n of friends)
                        sex = V(g.simple)$sex) # and their sex
rownames(data) <- c()

# Remain consistent in choosing random seeds in several of the following steps.
  # We simulate values (cups of coffee) via known distributions (in this case something called a Poisson distribution).
set.seed(123)
data$n_cups<- rpois(nrow(data), 7) # We use the Poisson distribution to simulate the amount of cups of coffee.
                                    # The distribution has only one parameter, "lambda", corresponding to the 
                                      # expected (mean) number.
set.seed(123)
data$age <- rpois(nrow(data), 35) # And the same to simulate age
data$n_sex <- ifelse(data$sex == "male", 0, 1) # We turn sex into a 0/1.
data 


#### 5.1 DESCRIPTIVE STATISTICS ----
  # Put simply, descriptive statistics describe the sample at hand
  # Among the most important of these values are those that describe the CENTRAL TENDENCY and DISPERSION
    # of our data; such as the mean, the median, the standard deviation, or variance.
mean(data$n_cups)
median(data$n_cups)
Mode(data$n_cups)

# The "stat.desc()" function from the "pastecs" package
  # returns a number of descriptive statistics
stat.desc(data$n_cups)

## DISTRIBUTION ##
  # Distributions have a central role in all of statistics, both descriptive and inferential
    # They are means of representing how values amass around certain points,
      # which in turn indicates the probabilities of encountering specific values or ranges of values.
    # Ways of displaying and interpreting distributions depend on the type of data, generally whether
      # the numbers generated by some process we intend to observe and investigate are (or are expected to be)
      # discrete or continuous.
h <- hist(data$n_cups)
ggplot(data = data, aes(x = n_cups)) +
  geom_histogram(bins = max(h$breaks) + 1, 
                 color = "red", 
                 fill = "red", alpha = .2) +
  labs(x = "Number of cups per week", y = "Count") +
  theme_classic()
    # A discrete form is appropriate for our data, give that we know that the process
      # that generated the data was one that could only have generated discrete values - positive integers (Poisson)

  # Smoother density visualizations are more suitable for continuous values, those that can take any real number
ggplot(data = data, aes(x = n_cups)) +
  geom_histogram(aes(y = ..density..), bins = max(h$breaks) + 1, 
                 color = "#c4a3a3", 
                 fill = "#c4a3a3", alpha = .2) +
  geom_density(color = "#6f8bff", lwd = 1) +
  labs(x = "Number (if continuous)", y = "Density") +
  theme_classic()


## In our data, we observe some clear deviations from the mean:
ggplot(data, aes(y = n_cups, x = id)) +
  geom_hline(yintercept = mean(data$n_cups), col = "red", lwd = 1.5) +
  geom_point(size = 1.4, alpha = .3) + 
  geom_segment(aes(y = mean(n_cups), x = id, yend = n_cups, xend = id), color = "blue", alpha = .4) +
  theme_classic() +
  theme(legend.position = "none",
        panel.grid = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks = element_blank()) +
  labs(x = "", y = "")

## These deviations, or DEVIANCE, is what we use to calculate the measures of dispersion;
  # the STANDARD DEVIATION and VARIANCE

# We arrive to standard deviation through the TOTAL ERROR:
options(scipen = 999)
  # The total error is the sum of all differences between scores and the mean (the difference of each score and the mean)
total.error <- sum(data$n_cups - mean(data$n_cups))
total.error 
  # What we get is so small that it is practically zero
  # This measure is obviously not very good, as deviances tend to cancel each other out.

# In order to counter the effect of errors cancelling each other out, we square the differences between scores and the mean
  # This is called the SUM OF SQUARED ERRORS (SS):
sum.squared <- sum((data$n_cups - mean(data$n_cups))^2)
sum.squared

## VARIANCE ## 
# Of course, this number - the sum of squared errors - will depend on the amount of data we have: 
  # the more errors we have (i.e. observations), the larger the Sum of squared errors
# To get a better feel for how well the mean describes our data and scores are dispersed around it, we use the average error
  # We average the error by getting SS divided by the number of observations
  # And this is VARIANCE:
sum.squared / nrow(data) # For the sample
sum.squared / (nrow(data) - 1) 
# The bottom value that uses the number of observations - 1 is the variance for the population
  # N-1 is due to degrees of freedom, a crucial concept in inferential statistics

# If we check our calculations, we get the same result
var(data$n_cups) # var() is the R function that returns variance
  # Or return to:
stat.desc(data$n_cups)
    # and look under "var".

## STANDARD DEVIATION ##
  # Variance gives us the average error in our data, but it returns it in units squared
  # So, to rectify this, we take the square root of this number, and get the STANDARD DEVIATION
sqrt(sum.squared/(nrow(data)-1))
sd(data$n_cups) # We indeed get the same result by using the sd() function


#### 5.2 CENTRAL LIMIT THEOREM ----
  # One of the central concepts of statistics that allows us in many cases to generalize our findings in one sample
    # to the population is the CENTRAL LIMIT THEOREM

  # The CLT tells us that, in many cases, the SAMPLING DISTRIBUTION of a (normalized) sample mean will tend towards
    # the (standard) normal distribution, even if the original data or population are not normally distributed, and 
    # particularly if the samples are large enough.

  # In more formal terms, the CLT says that as n (sizes of the sample) get larger (approach infinity),
    # the sample mean converges towards the population mean, or that the limit of the distribution of
    # sqrt(n)*(sample_mean - true_mean) is the standard normal distribution, with mu = 0, and sd = 1.

  # Let's try to demonstrate this.

## SAMPLING DISTRIBUTION OF SAMPLE MEANS AND CONVERGENCE TOWARDS CENTRAL LIMIT ##
# Although the wording may be slightly confusing, this is a fairly intuitive concept
  # This is simply the distribution of some measure (in this case the mean) that we find in all of our samples

# Let's simulate a population of coffee drinkers
set.seed(505)
population <- rpois(10000, 7)

  # And then draw 1000 samples of different sizes (10, 30, 100)
sample10 <- numeric(1000)
for (i in 1:1000) {
  sample10[i] <- mean(sample(population, 10, F))
}

sample30 <- numeric(1000)
for (i in 1:1000) {
  sample30[i] <- mean(sample(population, 30, F))
}

sample100 <- numeric(1000)
for (i in 1:1000) {
  sample100[i] <- mean(sample(population, 100, F))
}

plot(density(sample100), xlim = c(4, 10))
lines(density(sample30))
lines(density(sample10))
  # Observe how the sampling distribution of sample means converges towards the true mean of the population
    # the larger our samples get.
    # In other words, the larger the sample, the better the sample mean estimates the population mean.
    # This is crucial in investigating how a particular value compares to the overall distribution of values,
      # which is a concept that is at the centre of statistical significance testing.

  # We can demonstrate the convergence towards the true population mean
    # (or the convergence towards zero difference) as follows:
set.seed(123)
means <- numeric(500)
variances <- numeric(500)
samples <- list()
for (i in seq_along(seq(1, 1000, 1))) {
  samples[[i]] <- replicate(500, sample(population, size = i, replace = F))
  means[i] <- mean(samples[[i]])
  variances[i] <- var(samples[[i]])
}

mp1 <- ggplot(data = data.frame(x = seq(1, 1000, 1), y = means),
              aes(x = x, y = y)) + 
  geom_line(color = "blue", alpha = .4) +
  geom_hline(yintercept = mean(population), color = "red", lwd = .7) +
  labs(x = "Sample size", y = "Mean") +
  theme_classic()

mp2 <- ggplot(data = data.frame(x = seq(1, 1000, 1), y = variances),
              aes(x = x, y = y)) + 
  geom_line(color = "purple", alpha = .4) +
  geom_hline(yintercept = var(population), color = "blue", lwd = .7) +
  labs(x = "Sample size", y = "Variance") +
  theme_classic()

mp3 <- ggpubr::ggarrange(mp1, mp2, nrow = 2)
mp3
  # Mean and variance converge on the mean and variance of the population.


## STANDARDIZATION, Z-SCORES, AND CONFIDENCE INTERVALS ##
# Given what we have learned so far about the statistics of dispersion and the central limit theorem,
  # we can gain some insight into how what we measure in our sample relates to other samples we might have taken
  # and therefore to the population as a whole.

  # z-scores, when it comes to sample statistics, are values that indicate how far the observation of interest is from the mean
    # in terms of standard deviations (in other words, how many standard deviations is an observation from the mean of the sample)
    # and it is calculated as: z = (x - mean(x)) / sd

# For one of our observations in cups of coffee, say observation in row 15:
(data$n_cups[15] - mean(data$n_cups)) / sd(data$n_cups)

# This means that we can represent each observation in our data not as absolute values we observe, 
  # but rather as values measured in z-scores.
# This process of turning each observation into its z-score is called STANDARDIZATION. 
  # It turns whatever mean and standard deviation we may have in our original data into the mean of 0 and standard deviation of 1.

# What this means is that we can turn any data into a standardized normal distribution with the mean of 0 and sd of 1,
  # just by calculating the z-scores of observations.
data$cup_z_scores <- (data$n_cups - mean(data$n_cups))/sd(data$n_cups)
mean(data$cup_z_scores) # and we indeed have a mean of zero (it is not perfect zero simply because we limit our irrational numbers)
sd(data$cup_z_scores) # and a standard deviation of 1

# Let's see the distribution
plot(density(data$cup_z_scores))
  # We see that the distribution is centred around the mean of zero now, rather than the original raw mean.

# Remember that our sample, whatever we're studying, is only one of numerous samples we may have taken
# Our sample statistic, such as the sample mean we worked with above, is only one of numerous sample means we may have obtained
# Given that we know from the Central Limit Theorem that the sample means tend to be distributed normally (given sufficiently large samples), 
  # we can observe our sample mean as merely one of numerous sample means distributed along this normal distribution.

# We can therefore apply the same statistics we applied in the sample descriptive statistics to obtain the z-score for our sample mean.
# But instead of using the standard deviation of our data, we must of course use the standard deviation of the sampling distribution 
  # of the sample means (all samples).
  # And this we call the STANDARD ERROR, which we estimate as follows:

    # SE = s / sqrt(n), where s is the sample standard deviation.

# In our data, the standard error of the mean will be:
sd(data$n_cups) / sqrt(nrow(data)) # 0.148
# And indeed this is so:
stat.desc(data$n_cups) # Look at the number under SE.mean


  # Rather than estimating the error, let's sample from the population of coffee drinkers and calculate the z-score of
    # this one sample mean:
set.seed(123)
coffee <- sample(population, 50, replace = T)

mean(population) # The mean we observe in the population is 7.02
(mean(coffee) - mean(population)) / sd(population) # The z-score of this particular sample mean is -0.07
                                                   # The score is low, and therefore the sample mean is close to the true mean.

# Note, furthermore, that 95% of z-scores will fall between 1.96 and -1.96 SDs, which are roughly two (1.96) standard deviations,
  # and 99% will fall between 2.58 and -2.58.
# Given that our mean falls only -0.07 away from the mean of the distribution of sample means, 
 # this means that finding a sample with the mean that we observe is fairly likely.
  # The sample mean falls roughly in the middle of the distribution.

# This means that there is absolutely nothing exceptional about the mean of the sample we observe
# It is something that you would expect to find by random chance.

# Had we found a score that is so very unlikely under the assumption of randomness, then we would have 
  # something what we call a statistically significant observation, in which there are seems to be
    # some effect that pushes the mean out of what is considered the expected range. 
# The lower the probability, in other words, of obtaining this result, the greater our confidence that
  # this cannot have happened by pure chance, but through some influence that we then need to investigate.
# This phenomenon is at the basis of all frequentist statistics.

# In inferential statistics, it is usually some sort of regression coefficient that we are investigating.
# Regression coefficients, under the assumptions of specific statistical models, also have known standardized distributions,
  # as do the means under the Central Limit Theorem.
# Just as with the means, there too we can calculate the z-scores, t-values, and other types of standardized coefficients,
  # which tell us how likely or unlikely our result is, and how confident we are in rejecting the null hypothesis.
# The NULL HYPOTHESES, moreover, usually (but not necessarily always) assume that the effect is zero - that there is no effect.
  # So, with regression coefficients, for instance, we need to find how different our coefficient is from zero,
    # i.e. how exceptional our coefficient is in a null distribution with mean zero. 
# The z-scores or t-values are therefore calculated against the mean of zero, divided by the SE.

# Another important concept in statistics is the Type I error and the associated alpha-level
# Type I error is the error we commit when we reject the null hypothesis when it is in fact true.
  # Put differently, this happens when we find a significant effect when in reality there isn't one in the population.

# The alpha-level is the probability of committing a Type I error.
# This probability is usually set to 5% by convention. There is nothing intrinsically correct about 5%, 
  # it's just that this is a fairly small probability. If we choose so, and/or the research dictates it,
  # we may as well choose 10% as the alpha value we wish to use.
  
# Conversely, Type II error is an error we commit when we fail to reject a null hypothesis that is not true.
  # Put differently, we find no effect when, in reality, there is one in the population.

# The chosen alpha-level is furthermore related to CONFIDENCE INTERVALS:
  # Since we know that in the standard normal distribution, 95% of all observations will fall between -1.96 and 1.96
    # that is that 2.5% will fall below -1.96 and 2.5% above 1.96, 
    # we can calculate the interval that will in 95% of cases of the sample means (or whatever other statistics in other scenarios),
    # contain the TRUE MEAN/STATISTIC OF THE POPULATION. 

# Beware of the wording here. This does not mean that we are 95% confident that the mean is there,
  # but that if we take 100 samples, 95 of them will have their own confidence intervals and these 95 will
  # contain the true mean within their bounds.
# This translates to the fact that once we calculate the confidence interval for our observed statistic,
  # there is a 95% chance that it does indeed contain the true mean, and, remember, a 5% chance
    # that it does not, i.e. there is a  5% chance of us committing a Type I error.

# For the standard normal distribution, we calculate the confidence intervals in the following way:
# Lower CI: mean(x) - (1.96 * SE)
# Upper CI: mean(x) + (1.96 * SE)

# 95% confidence intervals for the mean of our cups of coffee per week:
standard.error <- (sd(coffee) / sqrt(50))
mean(coffee) - (1.96 * standard.error) # The lower bound is 6.18
mean(coffee) + (1.96 * standard.error) # The upper bound is 7.50

# 95% confidence interval (one of them) is therefore between 6.18 and 7.50, 
  # so there is a 5% chance that this interval does not contain the true mean of the population.
# And this is, then, how we interpret the confidence intervals:
  # The confidence interval based on the observed mean is one of possible intervals
    # of which 95% will contain the true mean.
# It follows that if our sample mean represents the true mean well, then the confidence interval of that mean will be small.
# Given that we know what the population mean is, we know that in this particular case,
  # the interval indeed does contain the true population mean.

# You may wonder why shouldn't we automatically use a smaller alpha value, say 1% or even 0%.
# It follows from what we have done so far that if we lower the alpha value, confidence interval must grow.
mean.model <- lm(coffee ~ 1)
confint(mean.model, level = 0.95) # This is a function that calculates the CIs for a linear model, in our case just the mean

# Let's try a 99% CI:
confint(mean.model, level = 0.99) # Now the confidence interval is wider.

# If we try 100%:
confint(mean.model, level = 1) # We get -infinity and +infinity
# And of course we do, because if we want to be absolutely certain that our confidence interval
  # contains the true mean of the population, then this interval must include all numbers in the universe.
  # Calculating a 100% CI is therefore useless.

## As a final thought, let's just quickly observe what is there in our linear model of the mean.
  # This will help us understand how all that we have said so far works with regression coefficients:
summary(mean.model)
# You see that the estimate of 6.84 is exactly our mean, and you see that instead of a z-score,
  # the linear models use something called a t-value, but this is conceptually the same thing.
# There is a t-distribution against which we can measure how likely or unlikely it is that we find our coefficient under the Null Hypothesis.
# The null hypothesis, remember, is that the effect is zero - or in this particular case that the true mean in the population is zero.
# Since the p-value (the thing under Pr(>|t|)) is so low, practically zero, it turns out that 
  # if the true mean of the population were indeed 0 and from our sample we get 6.84, then this 6.84 would be 
  # so immensely improbable that our confidence that it is not really zero (i.e. our confidence in rejecting the null hypothesis)
  # is nearly 100%, or to be precise higher than 99.9999999999999998%. 
# Since we have actually simulated this data, we know that the true mean is 7.025,
  # that is - not zero, and this corroborates what the inferential statistics are all about.


#### 5.3. STATISTICAL MODEL, COVARIANCE, AND CORRELATION ----

## COVARIANCE ##
# Covariance tells us whether and how two variables co-vary
  # As one variable deviates from the mean, does the other variable deviate similarly 
  # (or to the opposite direction) per observation?

# What we need then is to multiply the deviations of one variable by the corresponding deviations of the other
# These are known as CROSS-PRODUCT DEVIATIONS
# As with variance, we divide this by n-1 to get the average combined deviations
# When we do so, we get COVARIANCE

# Let's try calculating the covariance of cups of coffee and age of subjects
data
cross.dev <- sum((data$n_cups - mean(data$n_cups))*(data$age - mean(data$age)))
cross.dev # We have a cross-product deviation of -133.08

covariance <- cross.dev/(nrow(data)-1)
covariance # We have a negative covariance of -0.477
# The negative covariance tells us that as one variable deviates from the mean, the other deviates in the other direction.
# Here, however, the value is fairly low, that we can barely observe it in a scatter plot:
plot(data$age, data$n_cups)

cov(data$age, data$n_cups) # and we get the same result from the function.

## CORRELATION ##
# Covariance is NOT STANDARDIZED, so it will depend on the measures used 
# To standardize it, as before, we need to divide the value by the two standard deviations of our two variables - similar to z-values 
# When we standardize the covariance in this way, we get what is called the Pearson's CORRELATION COEFFICIENT

p.correlation <- cross.dev / ((nrow(data)-1) * sd(data$age) * sd(data$n_cups))
p.correlation # We have a small negative Pearson's correlation coefficient of -0.03, basically no correlation.

cor(data$age, data$n_cups) # and we get the same result from the function

## Squaring the correlation coefficient r, we get R^2, which in simple terms shows
  # how much of the variance in one variable is accounted for in the other
p.correlation^2 # very, very little in our case, 0.001S
cor(data$age, data$n_cups)^2


## SEMI-PARTIAL CORRELATION ##
# As an introduction to statistical modeling, let us consider for a moment the measure of partial correlation
# If we had multiple variables in our data - more than two, which you usually will have - 
  # then we might ask about the correlation between two of these variables, while controlling for the effect of the third variable.

# Let's create a new data set of coffee drinkers to demonstrate this.
  # We create the new data in order to plant a positive correlation there on purpose, to show how partial correlation works.
set.seed(123)
coffee <- rnorm_multi(250, 3, mu = c(35, 7, 4), sd = c(5.8, 2.13, 2), 
                           r = c(-0.3, 0.5, 0.5))
coffee <- round(coffee, 0)
coffee <- abs(coffee)
names(coffee) <- c("age", "coffee", "friends")

coffee # So, we have a dataset of 250 observations, with age, number of friends, and cups of coffee per week as variables.

cor(coffee$age, coffee$coffee) # Between age and coffee, we have a negative correlation of -0.21
# In other words, in our sample we observe that as age goes up, cups of coffee go down (or the other way round).
cor(coffee$age, coffee$coffee)^2 # 0.048, so around 5% of variance in coffee is accounted for by variance in age.

cor(coffee$friends, coffee$coffee) # A positive correlation of 0.52 between the number of friends and cups of coffee, 
  # that is - the more friends they have, the more coffee they drink.
cor(coffee$friends, coffee$coffee)^2 # And here the R^2 is 27.5%

# But let's now ask what happens to the correlation between age and cups of coffee when we control for the effect of friends.
# Put differently, if all of our subjects had the same number of friends 
  # - and so we filter out the effect of one's number of friends on their coffee-drinking habits - 
  # how then does this correlation between age and coffee behave?
m1 <- lm(coffee ~ friends, data = coffee) # First, let's rid coffee of the variance explained by friends
r <- m1$residuals # Get the residuals from the model, the bit of variance still left after
                    # accounting for the effect of the number of friends.
  # Now, we need to observe the correlation between these residuals and age,
    # in order to get the idea how cups of coffee correlate with age, after we have
      # "removed" the effect of friends.
cor(r, coffee$age)
    # The correlation is stronger still, and negative.

# This is the basis of understanding how statistical models work in explaining variances in dependent variables.


## STATISTICAL MODEL ##
## All statistical models, including regressions, can be written in the following form:

  # outcome[i] = (model) + error[i]

## Let's consider the Mean as a model of our data
  # outcome[i] = mean + error[i] (where [i] indicates a row of data)
mean(coffee$friends) # the mean of the number of friends in our new data is 4.09
coffee$friends[3] # Person number 3 in our data has 6 friends

# So, the number of friends at observation number three can be modeled as the mean + some error
# This error is the difference between the model (mean in this case) and what we observe
# This is the same, of course, as the error we encountered earlier
# In terms of linear regression (or any regression, for that matter) these errors are called RESIDUALS,
  # which we have met above when dealing with semi-partial correlation.
mean(coffee$friends) + (coffee$friends[3] - mean(coffee$friends))
# If we then write our observation as predicted from the mean, we of course get the correct number: 6.


## SIMPLE LINEAR REGRESSION ##
# The simple linear regression is a linear model, expressed in terms of linear equations,
  # that - by using the method of ORDINARY LEAST SQUARES - estimates the coefficients by which one
  # explanatory (independent) variable influences the behavior of the outcome (dependent) variable.

# Again, we can define the model with the mean in terms of the linear model
  # outcome.i = mean + error.i, which in the parlance of linear equations turns to:

# y = b0 + b1*X1 + error, where b0 is the intercept, and b1 is the effect of the one explanatory variable.

# In this model we only have the mean, so just the intercept
mean.model <- lm(coffee ~ 1, data = coffee) # using only 1 as the explanatory variable returns just the intercept, which is the mean
summary(mean.model) # what we have is the mean, its SE, and the t-statistic.
# Remember, as always, to standardize the estimate, we divide it by SE (just like we did with z).
# Also remember from above that here we are testing whether this coefficient (in this case the mean) is different from zero. 
# To calculate the statistic, therefore, we take the coefficient, subtract the zero from it, and divide it by its standard error.
7.18 / 0.1361 # This is exactly the same as the t-value we get from the model.

# We then find this standardized value in the known standardized distribution of t,
  # for the appropriate degrees of freedom (249, that is k-1):
https://www.itl.nist.gov/div898/handbook/eda/section3/eda3672.htm

# And find that the probability of getting this result if the true mean were zero would be virtually zero (p-value).s

## Let's try adding our one explanatory variable, for instance age:
model0 <- lm(coffee ~ age, data = coffee)
summary(model0) # We get a significant result for the effect of age.
# Notice that the estimate for the intercept (mean) also changes, and this is precisely due 
  # to the same reasons we discussed above in relation to the partial correlation.
# Once we try to also account for the effect of age in how cups of coffee vary, 
  # all other effects must change.


### MULTIPLE LINEAR REGRESSION
# The multiple linear regression is a linear model, expressed in terms of linear equations,
  # that - by using the method of ORDINARY LEAST SQUARES - estimates the coefficients by which several
  # explanatory (independent) variables influence the behavior of the outcome (dependent) variable.

# We can therefore add further independent (or predictor) variables to our models
# Now the statistical model becomes: 
  # y = b0 + b1*X1 + b2*X2... + bn*Xn + error

scatterplotMatrix(coffee)

model1 <- lm(coffee ~ age + friends, data = coffee)
summary(model1)
  # We find significant effects for both age and friends, with a negative effects for age
    # and positive for friends.
    # This means that, friends being equal, coffee cups drank per week decrease with age, 
      # and age held equal, they increase with the number of friends.

  # Let us plot what the model predicts with respect to age, with friends held constant:
new.data <- data.frame(age = seq(22, 54, 1), friends = mean(coffee$friends))
prediction <- predict.lm(model1, newdata = new.data, interval = "confidence", level = .95)
prediction <- as.data.frame(prediction)
prediction$age <- new.data$age

pred.plot <- ggplot(data = prediction, aes(x = age, y = fit)) +
  geom_line() +
  geom_ribbon(aes(x = age, ymin = lwr, ymax = upr), fill = "grey30", alpha = .2, col = "black", 
              lty = 2) +
  labs(x = "Age", y = "Cups of coffee") +
  theme_classic()
pred.plot

  # Our model, then, states that:
    # cups of coffee[i] = 12.28 - 0.25*Age[i] + 0.94*Friends[i] + error[i]

  # For someone with the average number of friends (4.092), aged 35,
    # the model predicts:
12.28696 + 35*-0.25705 + 4.092*0.94786
    # 7.168 cups of coffee per week.
summary(model1)
  # The adjusted R^2 in the output tells us how much of the variance does the model explain,
    # and we are at 58.73%.

# The Analysis of Variance Test (which we discuss in detail further down)
  # furthermore tells us that our model fits the data significantly better than the basic model (mean),
  # and a model with both predictors fits better still.
anova(model1)

summary(model0); summary(model1) # Higher R^2 indicates a better model, yet R^2 rises
                                  # with each additional predictor.
AIC(model1); AIC(model0) # Akaike Information Criterion is therefore a better
                          # measure, as it penalizes for excess predictors.
                          # AIC closer to zero indicates a better model.

res <- residuals(model1)
  # Residuals are the errors between the predicted and empirical data, left over
    # after we fit the model.
    # They are important in estimating the applicability and performance of any model.

  # If a model is correctly specified and performs well, then residuals are expected
    # to be independent and normally distributed around 0, without any clear patterns.
    # This is because we expect a well-performing model to have residuals that are both low
      # and free of any dependence or underlying correlations.
  # Observe diagnostic plots to identify potential issues with the model
    # or potential outliers in the data:
plot(model1)


# DIFFERENCE IN THE RESIDUAL SUM OF SQUARES ##
# In each model, we have TOTAL SUM OF SQUARES; RESIDUAL SUM OF SQUARES, and MODEL SUM OF SQUARES (Total SS - Residual SS):

# The TOTAL SS is the total error that we start with, between the mean and each observation
# The RESIDUAL SS is the error that takes into consideration the residuals, i.e. what of the error is left there after we apply our model
# The MODEL SS is the error that the model explains, that is the difference between the total and the residual SS

base.model <- lm(coffee ~ 1, data = coffee) # Just the mean as the model for the data.

TotalSS <- sum(base.model$residuals^2) # As before, we square the values to counter the fact that residuals tend to cancel each other out.
ResidSS <- sum(model1$residuals^2)
ModelSS <- TotalSS - ResidSS

# R^2 can here be calculated by calculating the ratio of explained error
R2 <- ModelSS/TotalSS # How much of the total variance are we accounting for
R2 # around 59%. This is the same as the "Multiple R-squared in the model summary:
summary(model1)

  # We can put the sums of squares further to work to run the ANOVA test by hand.
  # Let's compare model0 and model1 as earlier:
ResidSS_0 <- sum(model0$residuals^2)
ModelSS_0 <- TotalSS - ResidSS_0

  # Given that model0 has only age as the predictor,
    # the model SS here is the same as the corresponding SS in:
anova(model1)
    # Adding "friends" as the other predictor brings the 
      # model SS to 680. This comprises both age (55) and friends (625) SS.

  # To conduct the F-test in ANOVA, we proceed as follows:
MSS.delta <- ModelSS - ModelSS_0 # Get the difference in the Model SS of the two models.
        # This will naturally correspond, in this case, to the SS of the friends predictor.
    
  # We next calculate the model mean square by dividing the MSS by the difference in the degrees of freedom:
MS <- MSS.delta/1
  # Then proceed with calculating the Residual mean square by, similarly, dividing the 
    # residuals with the degrees of freedom, which are here n-k:
      # number of observations - total number of effects (including intercept)
MR <- ResidSS/(250-3)

  # The F-statistic is then simply:
F.stat <- MS / MR
  # The F-statistic here is 327.48
    # Given the numerator degrees of freedom = 1, and the denominator degrees of freedom = 247,
      # i.e. the degrees of freedom of MS and MR, we find the critical value in the F-table.
      # This value is far beyond the critical value for the alpha-level of 5%.
  # In other words, the model with friends and age as predictors fits far better than
    # the model with just age.

  # All of this, including numerator and denominator df, the mean squares and SS is displayed in
    # the function we've encountered earlier:
anova(model1)
  # The output tells us, just as we've found, that the model with age performs better
    # then just the mean, and the model with both age and friends performs better still.

## ASSESSING THE MODEL AND COEFFICIENTS ##
# It is always crucial to explore how well our models perform
# For various reasons (due to outliers, or skewed distributions, or errors in our assessment of assumptions of the models),
  # our models may perform poorly, and we estimate coefficients and significance that are erroneous.
# There are a number of ways to assess how well the models and each specific predictor perform
# We explore some of them here. This complements the diagnostics we have encountered alread.

# First, let's create a data frame with residuals, the Cook distance, and some other measures
resid <- data.frame(residuals = resid(model1),
                    standard.residuals = rstandard(model1),
                    student.residuals = rstudent(model1),
                    cook = cooks.distance(model1),
                    dfbeta = dfbeta(model1),
                    dffit = dffits(model1),
                    leverage = hatvalues(model1))

resid

# First let's inspect whether we have any outliers and influential cases
# Rules of thumb for standardized residuals are:
  # We should have less than 1% of SR larger than 2.58 (99th percentile)
  # If more than 5% have SR larger than 1.96 (95th percentile), we also might have a poor fit

# Let's count how many SR larger than 1.96 (2) do we have and find the OUTLIERS
resid$large.resid <- resid$standard.residuals > 2 | resid$standard.residuals < -2
sum(resid$large.resid) # We have 11 cases
11 / nrow(resid) # and that comes down to 0.044%, so we are good here.
resid[resid$large.resid,] # Let's see what these outliers are


# Looking for INFLUENTIAL CASES; those that drag the the model towards their value.
# dffit is the difference between what the model would predict if we exclude the observation and the originally predicted value
# If the case is not influential, then we'd expect this difference to be small

# We can get a similar measure by standardizing the difference between the ADJUSTED VALUE (the value predicted without the observation)
  # and the original observation - this is STUDENTIZED RESIDUAL

# Although these measures suggest how good the model is in predicting particular influential cases (when they are excluded)
  # these are not good measures to show how the influential cases influence the model as a whole

# Two statistics that do provide this information are COOK'S DISTANCE, and LEVERAGE:
# The former is the measure of the overall influence of the case on the model - values > 1 are cause for concern
# The latter - (k+1)/n - can range between 0 (no influence) to 1 (the specific case has complete influence over the prediction of the model)
# If no cases exert undue influence, we'd expect all hat values (leverage) to be close to the average leverage of (k+1)/n
# Values larger than 3 times the average cause concern

# Lastly, dffbeta is the difference between the coefficient of the model when the observation of interest is
  # excluded from the model and the coefficient in the model with that observation in. 
# This is a rather intuitive measure of the influence of each observation.

## Leverage. We have two observations larger than 3 times the average.
resid[resid$leverage > (4/nrow(resid))*3, ]
  # Other diagnostics related to these two cases are of no great concern.

## Checking Independence of Errors ##
# There must be no correlation among adjacent residuals - no "autocorrelation"
# Durbin-Watson test tests this assumption, but the results here will depend on the order of data!
durbinWatsonTest(model1) 
  # Our model appears fine, with no significant dependence.

## Checking Multicollinearity 
# There must be no high correlation between any two predictors, for obvious reasons
# If there is perfect correlation, then both explain the same variance
# This we test with the Variance Inflation Factors test
vif(model1) # Values larger than 10 indicate correlation - here around 1.3
mean(vif(model1)) # mean VIF substantially greater than 1 may pose a problem.

1/vif(model1) # Inverse of VIF is TOLERANCE.
              # Tolerance below 0.1 indicates a problem - no issues here.


#### 5.4 APPLYING APPROPRIATE LIKELIHOOD ----
  # Our investigation into the behaviour of the residuals, the model fit, and 
    # potential outliers indicates that we have a well-behaving model.
  
  # However, we have thus far been approaching the data through a linear model 
    # and Gaussian likelihood, which may not be an optimal approach for count data.
    # Linear, Gaussian models can sometimes work well, irregardless of the data, including
      # in cases of high, unbounded counts.
    # One immediate issue, however, that comes to mind is that counts cannot be negative integers,
      # nor can they be continuous.
      # Counts are positive, natural numbers.

  # Different likelihood distributions and classes of models are therefore applicable
    # to different scenarios, depending on the constraints placed on the outcome variable.
    # For the count data, given the constraints we've just mentioned, this model is the Poisson model, with the Poisson likelihood.

  # So, let us then re-run the analysis with the Poisson generalized linear model
model2 <- glm(coffee ~ age + friends, data = coffee, family = poisson)

new.data <- data.frame(age = seq(22, 54, 1), friends = mean(coffee$friends))
prediction1 <- predict(model2, newdata = new.data, se.fit = T, type = "response")
prediction1 <- as.data.frame(prediction1)
prediction1$age <- new.data$age
prediction1$lwr <- prediction1$fit - (1.96*prediction1$se.fit)
prediction1$upr <- prediction1$fit + (1.96*prediction1$se.fit)

pred.plot1 <- ggplot(data = prediction1, aes(x = age, y = fit)) +
  geom_line() +
  geom_ribbon(aes(x = age, ymin = lwr, ymax = upr), fill = "grey30", alpha = .2, col = "black", 
              lty = 2) +
  labs(x = "Age", y = "Cups of coffee") +
  theme_classic()
pred.plot1

summary(model2)

pchisq(deviance(model2), df.residual(model2), lower = F)
  # The model fits well (we are looking for a non-significant result here)
  
  # The standard Poisson mode, however, has only one parameter, lambda, which 
    # stands for the expected value as well as for variance (dispersion).
summary(model2) # Investigate the output for dispersion.

  # Our results, however, indicate that the assumptions about variance may be violated:
plot(log(fitted(model2)), log((coffee$coffee - fitted(model2))^2),
     xlab = expression(hat(mu)), ylab = expression((y - hat(mu))^2))
abline(0, 1)
  # The assumption seems violated, as the mean and variance do not mirror each other.
  # This means that, while the estimates are fine, the standard errors are biased, and
    # we cannot trust the model for inference.
  # We can, however, fit a specific dispersion parameter to fix this issue:
dispersion <- sum(residuals(model2, type = "pearson")^2 / model2$df.residual)
dispersion # The dispersion is 0.28.

  # Now we can adjust the standard errors:
summary(model2, dispersion = dispersion)
    # Notice how the estimates remain the same, but the p-values and the standard errors have changed.

    # We can achieve the same by running a quasi-Poisson model:
model2.1 <- glm(coffee ~ age + friends, data = coffee, family = quasipoisson)
summary(model2.1)

    
